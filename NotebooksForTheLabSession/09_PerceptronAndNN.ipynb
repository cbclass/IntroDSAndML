{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 9: The Perceptron and Neural Networks**\n",
    "\n",
    "Notebook for \"Introduction to Data Science and Machine Learning\"\n",
    "\n",
    "version 1.0, July 1 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required `import`-statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import` statements required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler,scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following code we suppress warning that some seaborn code will be deprecated in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings # To suppress some warnings\n",
    " \n",
    "# Suppress the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at **Neural Networks**. We will start by the neuron, the **perceptron**, the simple unit neural networks are composed of. A perceptron can be used to classify **linearly separable** data. We will use it to classify `and`. Then we will try to implement the `xor` operator. This is not possible, as xor is not linearly separable.\n",
    "\n",
    "We will then use a neural network to classify the `xor` operation. The network can be trained with standard parameters, but will become quite complex. Knowing more of the problem details, `xor` can be implemented in a simple network.\n",
    "\n",
    "After this we will look at the Iris and the Breast cancer data set. We will specifically:\n",
    "* see how to split data in \"normal\" and in stratified folds and observe the difference\n",
    "* split the data in trainings and test data\n",
    "* output the confustion matrix and accuracy\n",
    "* use normalization and observe the difference\n",
    "* use cross fold validation\n",
    "* test different network architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a data frame for the `and` operator.\n",
    "\n",
    "| `x` |  `y` |  `x and y` |\n",
    "|---|----|----|\n",
    "|`False`| `False`| `False`|\n",
    "|`False`| `True`| `False`|\n",
    "|`True`| `False`| `False`|\n",
    "|`True`| `True`| `True`|\n",
    "\n",
    "\n",
    "As the perceptron works with numerical input and output we replace `False`  by 0 and `True` by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "andData=np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])\n",
    "andFrame=pd.DataFrame(andData,columns=['x','y','and'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot it. To better see the points, we enlarge the displayed axis segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=andFrame.columns\n",
    "plt.Figure()\n",
    "sns.scatterplot(data=andFrame, x=cols[0], y=cols[1],hue=cols[2])\n",
    "plt.xlim((-.5,1.5))\n",
    "plt.ylim((-.5,1.5))\n",
    "plt.title('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the $X$ matrix (the features / samples) and the $y$ array with the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=andFrame.copy()\n",
    "y=X.pop('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the Perceptron and learn the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc=Perceptron(random_state=10)\n",
    "perc.fit(X,y)\n",
    "print(\"Weights:\",perc.coef_)\n",
    "print(\"Intercept:\",perc.intercept_)\n",
    "print(\"unique class labels:\",perc.classes_)\n",
    "\n",
    "ypred=perc.predict(X)\n",
    "print(ypred)\n",
    "print('Accuracy:',perc.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the function is linearly separable, the result is correct and learned quite fast:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate an `xor` data frame.\n",
    "\n",
    "| `x` |  `y` |  `x xor y` |\n",
    "|---|----|----|\n",
    "|`False`| `False`| `False`|\n",
    "|`False`| `True`| `True`|\n",
    "|`True`| `False`| `True`|\n",
    "|`True`| `True`| `False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xorData=np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,0]])\n",
    "xorFrame=pd.DataFrame(xorData,columns=['x','y','xor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols=xorFrame.columns\n",
    "plt.Figure()\n",
    "sns.scatterplot(data=xorFrame, x=cols[0], y=cols[1],hue=cols[2])\n",
    "plt.xlim((-.5,1.5))\n",
    "plt.ylim((-.5,1.5))\n",
    "plt.title('xor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that the data is not linearly separable. We try to learn the function by training the perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=xorFrame.copy()\n",
    "y=X.pop('xor')\n",
    "\n",
    "perc=Perceptron(random_state=10)\n",
    "perc.fit(X,y)\n",
    "print(\"Weights:\",perc.coef_)\n",
    "print(\"Intercept:\",perc.intercept_)\n",
    "print(\"unique class labels:\",perc.classes_)\n",
    "\n",
    "ypred=perc.predict(X)\n",
    "print(ypred)\n",
    "print('Accuracy:',perc.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function was not learned. In fact, `xor` cannot be learned by a simple perceptron as this function is not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A Neural Network for `xor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a neural network to learn a classifier for `xor`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a classifier and use it with standard parameter values. `MLPClassifier` stands for \"Multi-layer Perceptron classifier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier()\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a warning, that convergion has not yet been reached. The standard number of iterations is 200. We can add more iterations for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(max_iter=300)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test  different values for `max_iter` and try to get rid of the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained network has the standard architecture which consists of one hidden layer with 100 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network for `xor` can be built with one hidden layer with 2 nodes only, i.e. a much smaller network.\n",
    "\n",
    "Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(hidden_layer_sizes=(2,),random_state=10)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xor` was not corretly learned. The weights are not yet stable.\n",
    "\n",
    "Let's try some more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(hidden_layer_sizes=(2,),random_state=10,max_iter=1000)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again its not stable, so let's try more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(hidden_layer_sizes=(2,),random_state=10,max_iter=4000)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was finished but `xor` was not learned correctly. \n",
    "\n",
    "We know that `xor` can be classified using a neural network with the given architectre. So let's modify other parameters: \n",
    "- the activation function as well as the \n",
    "- solver algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(hidden_layer_sizes=(2,),random_state=10,activation='tanh',solver='lbfgs')\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "print(\"coefficients:\",nn.coefs_)\n",
    "print(\"intercepts:\",nn.intercepts_)\n",
    "ypred=nn.predict(X)\n",
    "print(ypred)\n",
    "print('Accuracy:',perc.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was correctly trained in 40 iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Please sketch a network with two nodes on the input layer, one hidden layer with two nodes and one node on the output layer **on a sheet of paper**.\n",
    "Assign the weights and intercepts to the sketch and test it by classiying the input `(1,0)` and `(1,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a neural network for the iris flower data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDS=load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a nice scatter plot using `pairplot()` from the `seaborn` module the data must be available in form of a pandas data frame. Therefore we create a data frame from the iris data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDF=pd.DataFrame(data=irisDS.data,columns=irisDS.feature_names)\n",
    "irisDF[\"class\"]=irisDS.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create and save the scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(irisDF,hue='class')\n",
    "plt.savefig('irisScatter.png',dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network with standard parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=irisDS.data\n",
    "y=irisDS.target\n",
    "\n",
    "nn=MLPClassifier(random_state=10)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network did not reach a stable state, so we increase the number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(random_state=10,max_iter=1000)\n",
    "nn.fit(X,y)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look at the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy:',nn.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated the accuracy value for the trainings data. This is of course not the best idea, as we aim to learn the classification of formerly unknown data. Therefore, we split trainings and test data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And repeat the training and look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(random_state=10,max_iter=1000)\n",
    "nn.fit(X_train,y_train)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "print('accuracy:',nn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture it might be important to \"stratify\" the sets, i.e. to make sure that the class disctribution of classes in the trainigs and test sets correspond to the class distribution of the original data set.\n",
    "\n",
    "We create stratified sets by using the additional parameter `stratify`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10,stratify=y)\n",
    "nn.fit(X_train,y_train)\n",
    "print(\"number of iterations:\",nn.n_iter_)\n",
    "print(\"number of weight updates:\",nn.t_)\n",
    "print('accuracy:',nn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the seed for the random number, this might lead to different results. Please note that the splitting is randomized, Therefore, the results might be better or worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to the accuracy we can calculate and print the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=nn.predict(X_test)\n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And equally create a colorful display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=irisDS.target_names)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Breast Cancer Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_breast_cancer()\n",
    "X=ds.data\n",
    "y=ds.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has a class attribute with two values, benign and malign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data in stratified trainings and test data, create a neural network with standard parameters, train it and output the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)\n",
    "\n",
    "nn=MLPClassifier(random_state=10)\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "ypred=nn.predict(X_test)\n",
    "print('accuracy:',nn.score(X_test,y_test)) \n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the process using stratified trainings and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10,stratify=y)\n",
    "\n",
    "nn=MLPClassifier(random_state=10)\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "ypred=nn.predict(X_test)\n",
    "print('accuracy:',nn.score(X_test,y_test)) \n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model did not yet converge, we increase the number of iterations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(random_state=10,max_iter=500)\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "ypred=nn.predict(X_test)\n",
    "print('accuracy:',nn.score(X_test,y_test)) \n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy was improved. \n",
    "\n",
    "Remember: The accuracy depends on the splits as trainings and test data are split randomly! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.33, random_state=12, stratify=y)\n",
    "nn=MLPClassifier(random_state=10,max_iter=1000)\n",
    "nn.fit(X_train2,y_train2)\n",
    "\n",
    "ypred2=nn.predict(X_test2)\n",
    "print('accuracy:',nn.score(X_test2,y_test2)) \n",
    "cm=confusion_matrix(y_test2,ypred2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will display min and max values of all the features using a loop in tabular form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):\n",
    "    print('{:25}: minimum: {:8.3f} maximum: {:9.3f}'.format(ds.feature_names[i],X[:,i].min(),X[:,i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the dimensions of the range of the features is quite different. As neural networks are sensitive to these differences, we **scale** the data using a z-score normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled=scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And output the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Xscaled.shape[1]):\n",
    "    print('{:25}: minimum: {:8.3f} maximum: {:9.3f}'.format(ds.feature_names[i],Xscaled[:,i].min(),Xscaled[:,i].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** As discussed in the lecture, trainings, validation and test data sets should be independent! By performing z-score scaling on the whole data set, we validate this requirement to a certain extend, as the parameters of the scaling base on the complete data set and therefore some information of the test data will be applied during scaling of the trainings data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this data set now to train and test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xscaled, y, test_size=0.33, random_state=10,stratify=y)\n",
    "\n",
    "nn=MLPClassifier(random_state=10,max_iter=1000)\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "ypred=nn.predict(X_test)\n",
    "print('accuracy:',nn.score(X_test,y_test)) \n",
    "cm=confusion_matrix(y_test,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the accuracy has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can still not make any valid conclusion about the accuracy, as we base on one split only. So we call cross validation and compare the results for the scaled and non scaled data. The function `cross_validate()` uses stratified folds if a classifier is trained and the problem is a binary / multiclass classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=MLPClassifier(random_state=10,max_iter=1000)\n",
    "\n",
    "results=cross_validate(nn,X,y)\n",
    "resultsUnscaled=results['test_score']\n",
    "print('Unscaled:', resultsUnscaled)\n",
    "\n",
    "results=cross_validate(nn,Xscaled,y)\n",
    "resultsScaled=results['test_score']\n",
    "print('Scaled:  ',resultsScaled)\n",
    "\n",
    "print('mean accuracy unscaled: {:.6f}, std: {:.6f}, var: {:.6f}'.format(resultsUnscaled.mean(),\n",
    "                                                                   resultsUnscaled.std(),resultsUnscaled.var()))\n",
    "print('mean accuracy   scaled: {:.6f}, std: {:.6f}, var: {:.6f}'.format(resultsScaled.mean(),\n",
    "                                                                   resultsScaled.std(),resultsScaled.var()))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we said that the network architecture can equally be trained. In the following code we test several different network architectures using a `for` - loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnLayers=[(100,),(50,),(20,),(50,20),(50,20,5)]\n",
    "\n",
    "results={}\n",
    "for hl in nnLayers:\n",
    "    nn=MLPClassifier(random_state=10,hidden_layer_sizes=hl,max_iter=1000)\n",
    "    results[str(hl)]=cross_validate(nn,Xscaled,y)\n",
    "\n",
    "for hl in results.keys():\n",
    "    res=results[hl]['test_score']\n",
    "    print('{:12}: mean accuracy: {:.6f}, std: {:.6f}, var: {:.6f}'.format(hl,res.mean(),res.std(),res.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in above results, more nodes or more layers do not necessarily lead to better results, based on the specific status of the random number gernerator.... $\\Rightarrow$ Many tests are required and (remember!) we only can get an estimate of the true accuracy within a confidence interval.\n",
    "\n",
    "In fact with the breast cancer data set we do not observe noticable differences. Depending on the data set different neural network architectures may lead to relevant differences in the results. So while we do not observe significant differences in this example, the methods can be applied to optimize neural networks and improve classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercise\n",
    "\n",
    "Load the penguiun data set and create a classifier for the `species` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "penguins=sns.load_dataset('penguins')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last notebook of the course some of the necessary steps to train a model were applied. Let's summarize them\n",
    "- data preparation: as neural networks are suscreptible to different ranges of variables, the values were scaled / normalized using the z-score scaling. As we apply this on the whole data set before any splits are performed, the trainings data will not be completely independent of test data.\n",
    "- hyperparameter settings: in the different examples we specified different hyperparameters such as the architecture, the solver algorithm and the activation function\n",
    "- we tested several hyperparameters using a for loop. Please note that we did not use a seperate validation set.\n",
    "- we used cross-fold va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*End of the Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This notebook was created by Christina B. Class for teaching at EAH Jena and is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1e76f8",
   "metadata": {},
   "source": [
    "**Section 3: Gradient Descent**\n",
    "\n",
    "Notebook for \"Introduction to Data Science and Machine Learning\"\n",
    "\n",
    "version 1.0, May 22 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e81f40",
   "metadata": {},
   "source": [
    "# Linear Regression: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106c586",
   "metadata": {},
   "source": [
    "In this lab we will implement the gradient descent step by step.\n",
    "\n",
    "We need the following modules. So please run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e55b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4ef8c",
   "metadata": {},
   "source": [
    "We initialize random with a fixed seed to make sure to get the same results when we run the code a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68426722",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7bf75",
   "metadata": {},
   "source": [
    "Gradient descent bases on the sum of squared errors as error measures. The following function calculates the sum of squared errors between the predicted values `ypred` and the measured values `y`. Both values are specied as `numpy` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumOfSquaredErrors(y,yPred):\n",
    "    errors=((y-yPred)**2).sum()\n",
    "    return errors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffdb26",
   "metadata": {},
   "source": [
    "The following function plots the real values, the regression function as well as the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regPlot(X,Y,Ypred,a,b, errors='',xlabel='x values', ylabel='y values'):\n",
    "    \"\"\" plot the result. you must specify vectors of the X and Y values as well as the predicted values\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    for i in range(len(X)):\n",
    "        plt.plot([X[i],X[i]],[Y[i],Ypred[i]],'k:',lw='.8')\n",
    "    plt.plot(X,Ypred,'.-')\n",
    "    plt.plot(X,Y,\".\")\n",
    "    text='$f(x)={:.3f}x+{:.3f}$'.format(a,b)\n",
    "    plt.title(text)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9eb893",
   "metadata": {},
   "source": [
    "# Learning the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3ef36",
   "metadata": {},
   "source": [
    "We start with the example of the flipped classroom exercise. The `X` and `Y` values are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2534a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class=np.array([1,1.5,3])\n",
    "Y_class=np.array([2.5 , 3.25, 5.5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0af33",
   "metadata": {},
   "source": [
    "As in the lecture and the video we start learning the intercept alone. We assume that the slope of the function is known. That is we need to learn the value $b$ for the following function: \n",
    "\n",
    "$f(x)=1.5 \\cdot x+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c3f52",
   "metadata": {},
   "source": [
    "Implement a Python function that calculates $f(x)$. Replace `return x` in the following function by the code calculating $f(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(x,b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005792f",
   "metadata": {},
   "source": [
    "To learn the intercept using gradient descent, we need the derivative with respect to the intercept of the sum of squared errors. The derivative is:\n",
    "\n",
    "$\\frac{\\partial}{\\partial_b} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot(y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot (y_{i} - \\hat{y_{i}}))$\n",
    "\n",
    "The value of the slope $a$ does not play a role in the derivative of the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f78a1",
   "metadata": {},
   "source": [
    "Implement a Python function to calculate the derivative of the intercept. Replace `pass` in the following function by the code to calculate the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSumOfSquaredErrorIntercept(y,ypred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31054c9c",
   "metadata": {},
   "source": [
    "Now we set the learning rate $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fd16f",
   "metadata": {},
   "source": [
    "We can use gradient descent to learn the intercept (the correct value is 1). The steps are the following:\n",
    "\n",
    "1. Start with a random value for the `intercept` (take 0)\n",
    "2. Determine the formula for the derivative of the loss function (of the sum of squared residuals)\n",
    "\n",
    "3. Calculate the derivative of the loss function for the actual intercept value: `interceptSlope` (this is the slope of the intercept curve)\n",
    "4. Calculate the `stepSize` as: `interceptSlope` times learning rate `alpha` \n",
    "5. Calculate the new `intercept` as: `intercept â€“ stepSize`\n",
    "6. go back to step 3. until either `abs(stepSize) < 0.001` (`abs()` is the absolute value) or the loop was executed 1000 times\n",
    "\n",
    "Implement gradient descent in the following function by adding the code to calculate the `stepSize` and adapt the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIntercept(X,Y,intercept=0,alpha=0.1):\n",
    "    stepSize=1\n",
    "    counter=0\n",
    "    while abs(stepSize)>=0.001:\n",
    "        yPred=function1(X,intercept)\n",
    "        interceptSlope=derivativeSumOfSquaredErrorIntercept(Y,yPred)\n",
    "        # calculate the stepSize\n",
    "        \n",
    "        # calculate the new Intercept\n",
    "        \n",
    "        # increment the loop counter\n",
    "        \n",
    "        if counter>1000:\n",
    "            break\n",
    "        \n",
    "    yPred=function1(X,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,1.5,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38464e",
   "metadata": {},
   "source": [
    "If you implemented the function correctly, the intercept of 1 was learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the code\n",
    "gradientIntercept(X_class,Y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a97cb",
   "metadata": {},
   "source": [
    "Now we run above code to learn the intercept (1) with other data. We use more `X` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=np.array(list(range(0,21)))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2=1.5*X2+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c3e1e",
   "metadata": {},
   "source": [
    "And now we add some random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ff58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2_rand=Y2+np.array([rnd.random()-0.5 for i in range(21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b774ddb",
   "metadata": {},
   "source": [
    "and create a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X2,Y2_rand,'.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c2cb1",
   "metadata": {},
   "source": [
    "Call the function to learn the intercept using gradient descent for `X` and `Yrand`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46660fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept(X2,Y2_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dbfd8",
   "metadata": {},
   "source": [
    "Now we get an error. If we remember the video, this might be due to a too large learning rate. So let's test a smaller learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept(X2,Y2_rand,alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc48825",
   "metadata": {},
   "source": [
    "In the video it was proposed to reduce the learning rate with every step to avoid above problems. Modify the gradient descent function by adapting the learning rate in each step (by multiplying it with 0.99):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIntercept2(X,Y,intercept=0,alpha=0.1):\n",
    "    stepSize=1\n",
    "    counter=0\n",
    "    while abs(stepSize)>=0.001:\n",
    "        yPred=function1(X,intercept)\n",
    "        interceptSlope=derivativeSumOfSquaredErrorIntercept(Y,yPred)\n",
    "        # calculate the stepSize\n",
    "\n",
    "        # calculate the new Intercept\n",
    "\n",
    "        # adapt the learning rate\n",
    "\n",
    "        # increment the loop counter\n",
    "\n",
    "        if counter>1000:\n",
    "            break\n",
    "        \n",
    "    yPred=function1(X,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,1.5,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8336f",
   "metadata": {},
   "source": [
    "Let's test, whether the problem accurs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept2(X2,Y2_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75516abb",
   "metadata": {},
   "source": [
    "# Learning the slope and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa06fd",
   "metadata": {},
   "source": [
    "Now we will implement gradient descent to learn the $a$ and $b$ of the function $f(x)=a \\cdot x + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925568c",
   "metadata": {},
   "source": [
    "Implement the function that calculates $a\\cdot x + b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b82878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(x,a,b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9dd1a",
   "metadata": {},
   "source": [
    "We now need the gradient, that is we need the derivate with respect to the interecpt for the sum of squared errors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1e24",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial_b} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot(y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot (y_{i} - \\hat{y_{i}}))$\n",
    "\n",
    "The value of the slope $a$ does not play a role in the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd56c8c",
   "metadata": {},
   "source": [
    "as well as with respect to the slope for the sum of squared errors.\n",
    "\n",
    "$\\frac{\\partial}{\\partial_a} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot x_i \\cdot (y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot x_i \\cdot (y_{i} - \\hat{y_{i}}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bf0b2",
   "metadata": {},
   "source": [
    "We already implemented the function `derivativeSumOfSquaredErrorIntercept(y,ypred)`. Now we implement the function `derivativeSumOfSquaredErrorSlope(x,y,ypred)`. This function requires the value of `x` as additional parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSumOfSquaredErrorSlope(x,y,ypred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bd813",
   "metadata": {},
   "source": [
    "The gradient descent algorithm is similar to learning the intercept alone:\n",
    "\n",
    "\n",
    "1. Start with a random value for the `intercept` and `slope` \n",
    "2. Determine the formula for the derivative of the loss function (of the sum of squared residuals)\n",
    "\n",
    "3. Calculate the derivative of the loss function with respect to the intercept for the actual slope and intercept values: `interceptSlope` (this is the slope of the intercept curve)\n",
    "4. Calculate the derivative of the loss function with respect to the slope for the actual slope and intercept value: `slopeSlope` (this is the slope of the slope curve)\n",
    "5. Calculate the `stepSizeIntercept` as: `interceptSlope` times learning rate `alpha` \n",
    "6. Calculate the `stepSizeSlope` as: `interceptSlope` times learning rate `alpha` \n",
    "7. Calculate the new `intercept` as: old intercept â€“ step size intercept\n",
    "8. Calculate the new `slope` as: old slope â€“ step size slope\n",
    "9. adapt the learning rate `alpha` by multiplying it with 0.99\n",
    "10. go back to step 3. until either both `abs(stepSizeIntercept) < 0.001` and `abs(stepSizeSlope) < 0.001` (`abs()` is the absolute value) or the loop was executed 1000 times\n",
    "\n",
    "Implement gradient descent in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac14ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with alpha 0.1 is too large, we set it to 0.01 by default\n",
    "def gradientInterceptSlope(X,Y,intercept=0,slope=0,alpha=0.01, precision=0.001):\n",
    "    stepSizeIntercept=1\n",
    "    stepSizeSlope=1\n",
    "    counter=0\n",
    "    while abs(stepSizeIntercept)>=precision or abs(stepSizeSlope)>=precision:\n",
    "        yPred=function2(X,slope,intercept)\n",
    "        # calculate the interceptSlope\n",
    "\n",
    "        # calculate the slopeSlope\n",
    "\n",
    "        # calculate the stepSizeIntercept\n",
    "\n",
    "        # calculate the stepSizeSlope\n",
    "\n",
    "        # calculate the new Intercept\n",
    "\n",
    "        # calculate the new Slope\n",
    "\n",
    "        # adapt the learning rate alpha\n",
    "\n",
    "        # increment the loop counter\n",
    "\n",
    "        if counter>1000:\n",
    "            break\n",
    "        \n",
    "    yPred=function2(X,slope,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"learned slope\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,slope,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca27a3",
   "metadata": {},
   "source": [
    "Now we call the function first with the standard value for precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7034bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientInterceptSlope(X_class,Y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba03c39",
   "metadata": {},
   "source": [
    "Remember that the function was defined $f(x)=1.5\\cdot x + 1$. Compare the learned intercept and slope with the values of the function. Take a look at the plot. You will see that the function was not precisely learned. Test out other values for precision. (The function call will be `gradientInterceptSlope(X_class,Y_class,precision=....)` with the new values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086982b-7a58-4cf9-bd98-61e60d37cf00",
   "metadata": {},
   "source": [
    "Please call gradient descent now using the correct intecept (1) so that the function only needs to learn the slope. Compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24757d-4b2e-4db8-af97-2e339e70acd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "gradientInterceptSlope(X_class,Y_class, intercept=1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f084a",
   "metadata": {},
   "source": [
    "Now run the code with the larger set of values (`X2` and `Y2`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81048d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "gradientInterceptSlope(X2,Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03405f",
   "metadata": {},
   "source": [
    "Test the gradient descent with other functions and values of your choice with and without random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2526119",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This notebook was created by Christina B. Class for teaching at EAH Jena and is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

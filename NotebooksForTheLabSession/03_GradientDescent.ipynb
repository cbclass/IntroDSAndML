{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1e76f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# **Section 3: Gradient Descent**\n",
    "\n",
    "Notebook for \"Introduction to Data Science and Machine Learning\"\n",
    "\n",
    "version 1.1, May 5 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e81f40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Linear Regression: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106c586",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this lab we will implement the gradient descent step by step.\n",
    "\n",
    "We need the following modules. So please run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e55b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b189551-39b6-47ee-b40b-c00dd1335c0f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And import some functions written for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3175f4a-2fc4-41f1-bf9b-b511136f54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.RegressionFunctions import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c886c4-b1ca-4e07-a224-692ac0d4975c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Gradient descent bases on the sum of squared errors as error measures, which is calculeted by the function `sumOfSquaredErrors()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sumOfSquaredErrors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffdb26",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The function `regPlot()` plots the values, regression line and residuals. Take a look at the information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f053f7-6fed-43a1-b348-c88415b8de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(regPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4ef8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We initialize random with a fixed seed to make sure to get the same results when we run the code a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68426722",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9eb893",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Learning the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3ef36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We start with the example of the flipped classroom exercise. The `X` and `Y` values are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2534a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class=np.array([1,1.5,3])\n",
    "Y_class=np.array([2.5 , 3.25, 5.5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0af33",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As in the lecture and the video we start learning only the intercept. We assume that the slope of the function is known. That is we need to learn the value $b$ for the following function: \n",
    "\n",
    "$f(x)=1.5 \\cdot x+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c3f52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Implement a Python function that calculates $f(x)$. Replace `return x` in the following function by the code calculating $f(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(x,b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005792f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To learn the intercept using gradient descent, we need the derivative with respect to the intercept of the sum of squared errors. The derivative is:\n",
    "\n",
    "$\\frac{\\partial}{\\partial_b} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot(y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot (y_{i} - \\hat{y_{i}}))$\n",
    "\n",
    "The value of the slope $a$ does not play a role in the derivative of the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f78a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Implement a Python function to calculate the derivative of the intercept. Replace `pass` in the following function by the code to calculate the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSumOfSquaredErrorIntercept(y,ypred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31054c9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we set the learning rate $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fd16f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can use gradient descent to learn the intercept (the correct value is 1). The steps are the following:\n",
    "\n",
    "1. Start with a random value for the `intercept` (take 0)\n",
    "3. Let `interceptSlope` be the derivative of the loss function for the actual intercept value\n",
    "4. Calculate the `stepSize` as: `interceptSlope` times learning rate `alpha` \n",
    "5. Calculate the new `intercept` as: `intercept â€“ stepSize`\n",
    "6. increment the loop counter\n",
    "7. go back to step 2. until either `abs(stepSize) < 0.001` (`abs()` is the absolute value) or the loop was executed 1000 times\n",
    "\n",
    "Implement gradient descent in the following function by adding the code to calculate the `stepSize` and adapt the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIntercept(X,Y,alpha=0.1):\n",
    "    # define initial intercept\n",
    "    \n",
    "    stepSize=1\n",
    "    counter=0\n",
    "    # define condition for while loop\n",
    "    while :\n",
    "        yPred=function1(X,intercept)\n",
    "        interceptSlope=derivativeSumOfSquaredErrorIntercept(Y,yPred)\n",
    "        # calculate the stepSize\n",
    "        \n",
    "        # calculate the new Intercept\n",
    "       \n",
    "        # increment the loop counter\n",
    "\n",
    "        \n",
    "    yPred=function1(X,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,1.5,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38464e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If you implemented the function correctly, the intercept of 1 was learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the code\n",
    "gradientIntercept(X_class,Y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a97cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we run above code to learn the intercept (1) with other data. We use more `X` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=np.array(list(range(0,21)))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2=1.5*X2+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c3e1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And now we add some random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ff58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2_rand=Y2+np.array([rnd.random()-0.5 for i in range(21)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b774ddb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "and create a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X2,Y2_rand,'.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c2cb1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Call the function to learn the intercept using gradient descent for `X` and `Yrand`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46660fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept(X2,Y2_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dbfd8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we get an error. If we remember the video, this might be due to a too large learning rate. So let's test a smaller learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept(X2,Y2_rand,alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc48825",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the video it was proposed to reduce the learning rate with every step to avoid above problems. Modify the gradient descent function by adapting the learning rate in each step (by multiplying it with 0.99):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45529906-dfc7-4a7e-8bff-61e596829bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientIntercept2(X,Y,alpha=0.1):\n",
    "    # define initial intercept\n",
    "    \n",
    "    stepSize=1\n",
    "    counter=0\n",
    "    # define condition for while loop\n",
    "    while :\n",
    "        yPred=function1(X,intercept)\n",
    "        interceptSlope=derivativeSumOfSquaredErrorIntercept(Y,yPred)\n",
    "        # calculate the stepSize\n",
    "        \n",
    "        # calculate the new Intercept\n",
    "       \n",
    "        # adapt the learning rate\n",
    "\n",
    "        # increment the loop counter\n",
    "\n",
    "        \n",
    "    yPred=function1(X,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,1.5,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8336f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's test, whether the problem accurs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientIntercept2(X2,Y2_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75516abb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Learning the slope and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa06fd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we will implement gradient descent to learn the $a$ and $b$ of the function $f(x)=a \\cdot x + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925568c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Implement the function that calculates $a\\cdot x + b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b82878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(x,a,b):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9dd1a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now need the gradient, that is we need the derivate with respect to the interecpt for the sum of squared errors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1e24",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "$\\frac{\\partial}{\\partial_b} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot(y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot (y_{i} - \\hat{y_{i}}))$\n",
    "\n",
    "The value of the slope $a$ does not play a role in the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd56c8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "as well as with respect to the slope for the sum of squared errors.\n",
    "\n",
    "$\\frac{\\partial}{\\partial_a} \\left( \\sum_{i} (y_i-(a  \\cdot x_i + b))^2 \\right) =  \\sum_{i} (-2 \\cdot x_i \\cdot (y_i-(a \\cdot x_i + b))) = \\sum_{i} (-2 \\cdot x_i \\cdot (y_{i} - \\hat{y_{i}}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bf0b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We already implemented the function `derivativeSumOfSquaredErrorIntercept(y,ypred)`. Now we implement the function `derivativeSumOfSquaredErrorSlope(x,y,ypred)`. This function requires the value of `x` as additional parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSumOfSquaredErrorSlope(x,y,ypred):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bd813",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The gradient descent algorithm is similar to learning the intercept alone:\n",
    "\n",
    "1. Start with a random value for the `intercept` and `slope`. We define default valzes of 0 in the function. \n",
    "2. Let `interceptSlope` be the derivative of the loss function with respect to the slope for the actual intercept value (assume slope to be given)\n",
    "3. Let `slopeSlope` be the derivative of the loss function  with respect to the slope for the atual slope value (assume intercept to be given)\n",
    "4. Calculate the `stepSizeIntercept` as: `interceptSlope` times learning rate `alpha` \n",
    "5. Calculate the `stepSizeSlope` as: `slopeSlope` times learning rate `alpha` \n",
    "6. Calculate the new `intercept` as: old intercept â€“ step size intercept\n",
    "7. Calculate the new `slope` as: old slope â€“ step size slope\n",
    "8. adapt the learning rate `alpha` by multiplying it with 0.99\n",
    "9. go back to step 2. until either both `abs(stepSizeIntercept) < 0.001` **and** `abs(stepSizeSlope) < 0.001` (`abs()` is the absolute value) or the loop was executed 1000 times\n",
    "\n",
    "Implement gradient descent in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac14ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with alpha 0.1 is too large, we set it to 0.01 by default\n",
    "def gradientInterceptSlope(X,Y,intercept=0,slope=0,alpha=0.01, precision=0.001):\n",
    "    stepSizeIntercept=1\n",
    "    stepSizeSlope=1\n",
    "    counter=0\n",
    "    # define the condition for while\n",
    "    while :\n",
    "        yPred=function2(X,slope,intercept)\n",
    "        # calculate the interceptSlope\n",
    "        \n",
    "        # calculate the slopeSlope\n",
    "        \n",
    "        # calculate the stepSizeIntercept\n",
    "        \n",
    "        # calculate the stepSizeSlope\n",
    "        \n",
    "        # calculate the new Intercept\n",
    "        \n",
    "        # calculate the new Slope\n",
    "        \n",
    "        # adapt the learning rate alpha\n",
    "        \n",
    "        # increment the loop counter\n",
    "        \n",
    "        \n",
    "    yPred=function2(X,slope,intercept)\n",
    "    print(\"Results\")\n",
    "    print(\"learned intercept\", intercept)\n",
    "    print(\"learned slope\", intercept)\n",
    "    print(\"sum of Squared errors\",sumOfSquaredErrors(Y,yPred))\n",
    "    print(\"loop counter: \",counter)\n",
    "    regPlot(X,Y,yPred,slope,intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca27a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we call the function first with the standard value for precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7034bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientInterceptSlope(X_class,Y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba03c39",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Remember that the function was defined $f(x)=1.5\\cdot x + 1$. Compare the learned intercept and slope with the values of the function. Take a look at the plot. You will see that the function was not precisely learned. Test out other values for precision. (The function call will be `gradientInterceptSlope(X_class,Y_class,precision=....)` with the new values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086982b-7a58-4cf9-bd98-61e60d37cf00",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Please call gradient descent now using the correct intecept (1) so that the function only needs to learn the slope. Compare the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24757d-4b2e-4db8-af97-2e339e70acd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f084a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now run the code with the larger set of values (`X2` and `Y2`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81048d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03405f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Test the gradient descent with other functions and values of your choice with and without random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2526119",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This notebook was created by Christina B. Class for teaching at EAH Jena and is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
